{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "First, construct Alphabet of all languages\n",
    "'''\n",
    "import math as m\n",
    "NEW_TRAIN_X = open(\"data/train_set_x_cleaned_nothing.txt\",\"r\")\n",
    "NEW_TRAIN_Y = open(\"data/train_set_y.txt\",\"r\")\n",
    "D = len(NEW_TRAIN_Y.readlines())\n",
    "NEW_TRAIN_Y.close()\n",
    "NEW_TRAIN_Y = open(\"data/train_set_y.txt\",\"r\")\n",
    "#alphabet for all languages\n",
    "Alphabet = []\n",
    "for lineY in NEW_TRAIN_Y:\n",
    "    lang = (lineY.split(\",\"))[1].replace(\"\\n\", \"\")\n",
    "    text = (NEW_TRAIN_X.readline()).split(\",\")[1].replace(\"\\n\", \"\")\n",
    "    chars = text.split()\n",
    "    for char in chars:\n",
    "        if char not in Alphabet:\n",
    "            Alphabet.append(char)\n",
    "NEW_TRAIN_X.close()\n",
    "NEW_TRAIN_Y.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from IPython.display import Image\n",
    "Image(\"img/TF-IDF.png\")\n",
    "![title](img/TF-IDF.png)\n",
    "Sourced cited from wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Second, construct the neighbourhood of vectors \n",
    "each vector contains the TF_IDF value of each unique letter in a sentence\n",
    "See the above formula\n",
    "'''\n",
    "NEW_TRAIN_X = open(\"data/train_set_x_cleaned_nothing.txt\",\"r\")\n",
    "NEW_TRAIN_Y = open(\"data/train_set_y.txt\",\"r\")\n",
    "#construct vector base on tf-idf\n",
    "neighbourhood = [[0 for i in range(len(Alphabet))] for j in range(D)] \n",
    "#also store document frequency\n",
    "df = [0]*len(Alphabet)\n",
    "#last element in the vector is used to store which language it is\n",
    "count = 0\n",
    "langID = [0]*D\n",
    "for lineY in NEW_TRAIN_Y:\n",
    "    lang = (lineY.split(\",\"))[1].replace(\"\\n\", \"\")\n",
    "    text = (NEW_TRAIN_X.readline()).split(\",\")[1].replace(\"\\n\", \"\")\n",
    "    chars = text.split()\n",
    "    #last element in the vector is used to store which language it is\n",
    "    langID[count] = int(lang)\n",
    "    for char in chars:\n",
    "        index = Alphabet.index(char)\n",
    "        #counting term frequency\n",
    "        neighbourhood[count][index] = neighbourhood[count][index] + 1\n",
    "        #make sure it is counted once per document\n",
    "        if(neighbourhood[count][index] == 1):\n",
    "            df[index] = df[index] + 1\n",
    "    count = count+1\n",
    "    \n",
    "for i in range(len(df)):\n",
    "    df[i] = m.log(abs(D)*1.0/df[i])\n",
    "for i in range(len(neighbourhood)):\n",
    "    for j in range(len(Alphabet)):\n",
    "        neighbourhood[i][j] = (neighbourhood[i][j] * 1.0) * df[j]\n",
    "NEW_TRAIN_X.close()\n",
    "NEW_TRAIN_Y.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Third, Train KNN classifier from Scikit Learn library\n",
    "NOTE: this step might cause memory error in some machines due to the large memory location required for the neighbourhood vector\n",
    "'''\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "neigh = KNeighborsClassifier(n_neighbors=3)\n",
    "neigh.fit(neighbourhood,langID) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Validate\n",
    "\n",
    "'''\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "validation = open(\"data/validation.csv\",\"r\")\n",
    "success = 0\n",
    "total = 0\n",
    "fail = 0\n",
    "for test in validation:\n",
    "    if(total%1000 == 0):\n",
    "        print(total)\n",
    "    lang = (test.split(\",\"))[0]\n",
    "    letters = ((test.split(\",\"))[1].replace(\"\\n\", \"\")).split(\" \")\n",
    "    vector = [0]*len(Alphabet)\n",
    "    for letter in letters:\n",
    "        try:\n",
    "            index = Alphabet.index(letter)\n",
    "        except:\n",
    "            pass\n",
    "        vector[index] = vector[index]+1\n",
    "    for i in range(len(Alphabet)):\n",
    "        vector[i] = (vector[i]*1.0)*df[i] \n",
    "    prediction = neigh.predict(vector)\n",
    "    if(prediction[0] == int(lang)):\n",
    "        success = success + 1\n",
    "        total = total + 1\n",
    "    else:\n",
    "        fail = fail + 1\n",
    "        total = total + 1\n",
    "print (success*1.0/total)\n",
    "    \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Self written KNN learner, not used in the report due to high computational time\n",
    "\n",
    "#remove language at end of the vector\n",
    "import numpy as np\n",
    "import scipy.spatial.distance as dis\n",
    "validation = open(\"../data/validation.csv\",\"r\")\n",
    "#use previously calculated df as idf for this case\n",
    "success = 0\n",
    "total = 0\n",
    "fail = 0\n",
    "for test in validation:\n",
    "    lang = (test.split(\",\"))[0]\n",
    "    letters = ((test.split(\",\"))[1].replace(\"\\n\", \"\")).split(\" \")\n",
    "    vector = [0]*len(Alphabet)\n",
    "    for letter in letters:\n",
    "        try:\n",
    "            index = Alphabet.index(letter)\n",
    "        except:\n",
    "            pass\n",
    "        vector[index] = vector[index]+1\n",
    "    for i in range(len(Alphabet)):\n",
    "        vector[i] = (vector[i]*1.0)*df[i]\n",
    "    mostFit = 10000000\n",
    "    position = 0\n",
    "    for i in range(len(neighbourhood)):\n",
    "        dist = dis.euclidean(vector,neighbourhood[i])\n",
    "        if(dist < mostFit):\n",
    "            mostFit = dist\n",
    "            position = i\n",
    "    if(int(langID[position]) == int(lang)):\n",
    "        success = success + 1\n",
    "        total = total + 1\n",
    "    else:\n",
    "        fail = fail + 1\n",
    "        total = total + 1\n",
    "        \n",
    "print(success*1.0/total)\n",
    "'''\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
