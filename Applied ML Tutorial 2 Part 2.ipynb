{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Tutorial Comp 551 - Part 2\n",
    "\n",
    "Sept 28th, 2017\n",
    "\n",
    "### Agenda\n",
    "\n",
    "- Text representation\n",
    "    - one hot\n",
    "    - embeddings\n",
    "    - pre-trained embeddings\n",
    "- Keras\n",
    "- Spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Spaced Representation of Documents\n",
    "\n",
    "A very simple approach to represent documents as numerical value is to use each word as an atomic type and as a basis for a vector space. For example imagine a world where there exist only 3 words: “Apple”, “Orange”, and “Banana” and every sentence or document is made of them. They become the basis of a 3 dimensional vector space:\n",
    "\n",
    "```\n",
    "Apple  ==>> [1,0,0]\n",
    "Banana ==>> [0,1,0]\n",
    "Orange ==>> [0,0,1]\n",
    "```\n",
    "\n",
    "This representation is called \"one_hot\" as it is always a vector of zeros with 1 on the position of the word.\n",
    "\n",
    "Then a “sentence” or a “document” is simply the linear combination of these vectors where the number of the counts of appearance of the words is the coefficient along that dimension. For example:\n",
    "\n",
    "```\n",
    "d3 = \"Apple Orange Orange Apple\" ==>> [2,0,2]\n",
    "d4 = \"Apple Banana Apple Banana\" ==>> [2,2,0]\n",
    "d1 = \"Banana Apple Banana Banana Banana Apple\" ==>> [2,4,0]\n",
    "d2 = \"Banana Orange Banana Banana Orange Banana\" ==>> [0,4,2]\n",
    "d5 = \"Banana Apple Banana Banana Orange Banana\" ==>> [1,4,1]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problems\n",
    "\n",
    "1. There are words that are just very common, so they appear in lots of documents. (“the”, “and”, “or” etc..)\n",
    "2. Huge vocabulary makes the vectors very sparse\n",
    "3. When words are used as atomic types for the basis of the vector space, they have no semantic relations (the similarity between them is zero, since they are perpendicular to each other). However, in reality we know that words can be similar in meaning, or even almost identical synonyms.\n",
    "4. There is no semantic structure incorporated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embeddings\n",
    "\n",
    "Here we create a \"dense\" representation of each word where proximity in vector space represents \"similarity\". \n",
    "\n",
    "![](https://github.com/michaelcapizzi/nlp-basics/raw/753ab4c178c6bd2cebcc8d4a5631bf6220c85479/images/king_queen_vis.png)\n",
    "\n",
    "The basic idea is, using a shallow neural network we can train on a large corpora of text to generate individual word vectors, which are located in closely related semantic space. Each word is representated by a distribution of weights across those elements. So instead of a one-to-one mapping between an element in the vector and a word, the representation of a word is spread across all of the elements in the vector, and each element in the vector contributes to the definition of many words.\n",
    "\n",
    "![](https://adriancolyer.files.wordpress.com/2016/04/word2vec-distributed-representation.png?w=566&zoom=2)\n",
    "\n",
    "In practice, we can either **train** the word embeddings from scratch on our dataset, or we can use **pre-trained** word embeddings. Gigawords and GloVe are two most popular word embeddings in use.\n",
    "\n",
    "Refer to these resources:\n",
    "\n",
    "- https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets start with text classification using some of the concepts we learnt before.\n",
    "We will be using the Keras library (https://keras.io/), but know that there are many more that may suit your personal needs and preferences better (Block/Fuel, Lasagne, MXNet, Torch, Caffe, Deeplearning4j (Java), among many!).\n",
    "\n",
    "You may want to go to a lower level (or implement fancier models), and directly use symbolic CPU&GPU computing libraries such as Theano, Tensorflow, or Chainer.   \n",
    "Note that Keras is built on top of Theano/Tensorflow, and requires one or the other as a backend."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataset** : We will be using Reuters newswire topic classification task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "from keras.datasets import reuters\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras import utils as np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_words = 1000\n",
    "batch_size = 32\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = reuters.load_data(nb_words=max_words,\n",
    "                                                         test_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8982"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2246"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we vectorize the input sentences. Here first we use a simple one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "x_train = tokenizer.sequences_to_matrix(x_train, mode='binary')\n",
    "x_test = tokenizer.sequences_to_matrix(x_test, mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  1.,  1., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  1.,  1., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  1.,  1., ...,  0.,  0.,  0.],\n",
       "       ..., \n",
       "       [ 0.,  1.,  1., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  1.,  1., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  1.,  1., ...,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert class vector to binary class matrix, for the categorical cross-entropy loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_classes = np.max(y_train) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8982, 46)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we proceed to build the model. Here we are using a simple feedforward neural network with Relu non-linearity. Using Keras you can easily build complex architectures on your own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(512, input_shape=(max_words,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training your model\n",
    "\n",
    "In order to train models, people usually use some variant of stochastic gradient descent. The good thing with all these libraries is that all you have to do is declare the architecture of your model, and the library takes care of automatically computing gradients for you!\n",
    "\n",
    "### Losses\n",
    "\n",
    "There are many loss functions one can use. The two basic ones you will need are **mean square error** (quite general)\n",
    "$$ \\frac{1}{N_{ex}}\\sum_i^{N_{examples}} \\|f(\\mathbf{x}_i)-\\mathbf{y}_i\\|^2_2 $$\n",
    "and **categorical crossentropy** (specialized for classification):\n",
    "$$ - \\frac{1}{N_{ex}}\\sum_i^{N_{ex}} \\sum_j^{N_{classes}} y_{i,j}\\log f_j(\\mathbf{x}_i) $$\n",
    "Here $y_i$ is assumed to be a 1-hot vector of the class encoding, but when implemented only the index of the 1 is necessary (\"sparse\" categorical crossentropy).\n",
    "\n",
    "### Gradient descent methods\n",
    "\n",
    "There are several GD methods you should know.\n",
    "\n",
    "The basic one, **SGD**, consists in taking a single example or a minibatch of examples and updating your weights a bit.\n",
    "$$\\theta \\leftarrow \\theta - \\alpha \\nabla_\\theta \\mathcal{L}$$\n",
    "The learning rate is $\\alpha$, typically $\\alpha<1$.\n",
    "\n",
    "**Momentum** consists in keeping track of the velocity of your stochastic gradient descent, so as to keep going in the same general direction, and can considerably speed up learning.\n",
    "$$\\begin{align} \\mu &\\leftarrow \\gamma \\mu + (1-\\gamma) \\nabla_\\theta \\mathcal{L} \\\\ \\theta &\\leftarrow \\theta - \\alpha \\mu\n",
    "\\end{align}$$\n",
    "With $0<\\gamma<1$, usually quite close to 1.\n",
    "\n",
    "**RMSProp** (Root Mean Square Prop) is similar in spirit, but is intended to act more like an adaptive learning rate.\n",
    "$$\\begin{align} g &= \\nabla_\\theta \\mathcal{L}\\\\\n",
    "\\eta &\\leftarrow \\gamma \\eta + (1-\\gamma) g^2 \\\\\n",
    "\\theta &\\leftarrow \\theta - \\frac{\\alpha g}{\\sqrt{\\eta + \\epsilon}}\n",
    "\\end{align}$$\n",
    "With $\\epsilon$ typically around $10^{-4}$\n",
    "\n",
    "**Adam** (Adaptive Moment estimation) is probably the most popular method right now, and is somewhat a mix of RMSProp and Momentum:\n",
    "$$\\begin{align} g &= \\nabla_\\theta \\mathcal{L}\\\\\n",
    "\\mu &\\leftarrow \\beta_1 \\mu + (1-\\beta_1) g \\\\\n",
    "\\eta &\\leftarrow \\beta_2 \\eta + (1-\\beta_2) g^2 \\\\\n",
    "\\theta &\\leftarrow \\theta - \\frac{\\alpha \\mu}{\\sqrt{\\eta + \\epsilon}}\n",
    "\\end{align}$$\n",
    "\n",
    "### Dropout\n",
    "\n",
    "Dropout is a regularization technique, where some proportion $p$ of the units is randomly multiplied by 0 at each new forward pass. This forces the model to be robust to noise and often to generalize better, as it cannot rely on a single unit to hold some vital information about the final prediction. It is usually only applied to intermediate layers, but can be applied to the input as well.\n",
    "\n",
    "For a more complete description of algorithms, see http://sebastianruder.com/optimizing-gradient-descent/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now compile the model. We use **categorical cross-entropy** loss, and **Adam** optimizer. Generally for NLP tasks Adam optimizer works quite good, but feel free to experiment on your own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model is ready to be trained!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8083 samples, validate on 899 samples\n",
      "Epoch 1/5\n",
      "8083/8083 [==============================] - 1s - loss: 1.4220 - acc: 0.6776 - val_loss: 1.0965 - val_acc: 0.7519\n",
      "Epoch 2/5\n",
      "8083/8083 [==============================] - 1s - loss: 0.7837 - acc: 0.8168 - val_loss: 0.9314 - val_acc: 0.8031\n",
      "Epoch 3/5\n",
      "8083/8083 [==============================] - 1s - loss: 0.5506 - acc: 0.8673 - val_loss: 0.8852 - val_acc: 0.7998\n",
      "Epoch 4/5\n",
      "8083/8083 [==============================] - 1s - loss: 0.4110 - acc: 0.8984 - val_loss: 0.8896 - val_acc: 0.8009\n",
      "Epoch 5/5\n",
      "8083/8083 [==============================] - 1s - loss: 0.3235 - acc: 0.9197 - val_loss: 0.9103 - val_acc: 0.7987\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just within 5 epochs, we reach 90% training accuracy as 79% validation accuracy!\n",
    "\n",
    "Now we evaluate the test set like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1536/2246 [===================>..........] - ETA: 0s"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test, y_test,\n",
    "                       batch_size=batch_size, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.89323368777362555, 0.79207479964381122]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where, the first element is the test score (mean loss), and the second element is the test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducing Spacy\n",
    "\n",
    "While `nltk` is a fairly comprehensive tool to do text processing and NLP tasks, in this tutorial we would like to introduce a new kid on the market, [Spacy](https://spacy.io). Written purely in Python and Cython bindings, Spacy gives us a whole new way to process text, get the word embeddings and can be plugged in to any existing deep learning framework with ease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en') # language model to load. Spacy comes with 3 language models : English, German, French, although alpha support is available for other languages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "742225"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nlp.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentence = nlp(u'hey how are you?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hey how are you?"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word = sentence[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hey"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spacy comes with pre-trained word2vec embeddings from Glove."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -2.84509987e-01,   3.10070008e-01,  -5.70389986e-01,\n",
       "        -7.30559975e-02,  -1.73219994e-01,   3.45140010e-01,\n",
       "         2.50640009e-02,  -7.44499981e-01,  -8.00030008e-02,\n",
       "         1.27059996e+00,  -1.54060006e-01,  -5.62049985e-01,\n",
       "        -5.52049987e-02,  -2.57739991e-01,  -3.04600000e-02,\n",
       "        -1.09810002e-01,   9.67290029e-02,   5.18400013e-01,\n",
       "        -9.26730037e-02,  -4.92510013e-02,   9.01570022e-02,\n",
       "         9.43889990e-02,   1.80559993e-01,  -6.19909987e-02,\n",
       "         7.09050000e-02,  -2.71360010e-01,  -8.50069970e-02,\n",
       "        -1.11780003e-01,   5.10959983e-01,   7.31770024e-02,\n",
       "        -7.37999976e-02,   4.16130006e-01,   4.70569991e-02,\n",
       "         5.62300012e-02,  -3.62500012e-01,   3.00779998e-01,\n",
       "        -5.41069992e-02,   1.59170002e-01,  -3.28060001e-01,\n",
       "         4.12040018e-02,  -7.02250004e-02,  -1.33489996e-01,\n",
       "        -2.04420000e-01,  -1.81180000e-01,   3.01580001e-02,\n",
       "         2.43729994e-01,   1.51639998e-01,  -1.19800001e-01,\n",
       "         3.40539992e-01,   1.44160002e-01,   2.19750002e-01,\n",
       "         3.80640000e-01,   3.77230011e-02,   1.30549997e-01,\n",
       "        -2.10140005e-01,  -1.04479998e-01,  -1.21699996e-01,\n",
       "         6.16599992e-02,  -2.73570001e-01,   4.77290004e-02,\n",
       "        -7.53939986e-01,  -4.28059995e-02,  -3.09410006e-01,\n",
       "        -2.12249994e-01,  -6.19219989e-02,  -4.85219985e-01,\n",
       "        -2.21369997e-01,   3.12169999e-01,   1.18969999e-01,\n",
       "        -3.54389995e-01,   2.23920003e-01,   8.98389984e-03,\n",
       "         2.45509997e-01,   1.01410002e-02,   1.04170002e-01,\n",
       "         3.35990004e-02,   2.38490000e-01,   2.31220007e-01,\n",
       "        -7.22789988e-02,   5.25690019e-01,   2.67220009e-02,\n",
       "         3.73360008e-01,  -2.70139992e-01,   2.28929996e-01,\n",
       "        -4.04409990e-02,  -9.93240029e-02,   1.26919997e+00,\n",
       "        -1.04369998e+00,  -5.43980002e-02,  -6.74000010e-02,\n",
       "         1.74889997e-01,  -2.20009997e-01,  -1.74659997e-01,\n",
       "         2.16700003e-01,   3.48870009e-01,  -1.72069995e-03,\n",
       "         1.63519993e-01,  -2.50110000e-01,  -5.23350000e-01,\n",
       "         1.06020004e-01,  -2.78689992e-02,  -1.03100002e-01,\n",
       "        -1.32029997e-02,   2.93480009e-01,   1.45769998e-01,\n",
       "        -1.46029994e-01,  -7.67270029e-02,  -3.47730011e-01,\n",
       "        -3.51819992e-01,  -2.16040000e-01,   1.95600003e-01,\n",
       "        -7.76970029e-01,   4.85920012e-02,  -1.83190003e-01,\n",
       "         2.67390013e-01,   5.66450000e-01,   8.21819976e-02,\n",
       "        -3.92549992e-01,  -4.39630002e-01,  -1.72549993e-01,\n",
       "         2.98159987e-01,  -4.83489990e-01,   2.26579994e-01,\n",
       "        -1.46009997e-01,   4.10950005e-01,   1.07460000e-01,\n",
       "         2.51819998e-01,  -3.25150013e-01,  -8.18809960e-03,\n",
       "         5.20810008e-01,   1.36310002e-02,  -3.58480006e-01,\n",
       "         2.32360005e-01,  -1.02130003e-01,   3.71749997e-01,\n",
       "        -3.56530011e-01,  -2.11140007e-01,  -1.08479999e-01,\n",
       "        -3.00839990e-01,  -2.44069993e-01,  -3.35260010e+00,\n",
       "         1.41330004e-01,   1.12740003e-01,  -4.78149988e-02,\n",
       "        -4.89749998e-01,   1.09540001e-01,  -1.15189999e-01,\n",
       "         4.70279992e-01,  -3.43149990e-01,  -1.81260005e-01,\n",
       "         1.24320004e-03,  -1.12960003e-01,  -4.14520018e-02,\n",
       "        -2.00250000e-01,   2.41669998e-01,   1.21050000e-01,\n",
       "         4.11190003e-01,  -4.60370004e-01,  -1.95800006e-01,\n",
       "        -7.98199996e-02,   9.27259997e-02,   8.52940045e-03,\n",
       "        -1.20729998e-01,   1.64630003e-02,  -5.20990014e-01,\n",
       "         6.94670007e-02,   1.89620003e-01,  -2.59419996e-03,\n",
       "         4.86540012e-02,   3.10250014e-01,  -1.31610006e-01,\n",
       "        -5.92439994e-02,  -1.43340006e-01,  -6.39829993e-01,\n",
       "        -7.49830008e-02,  -4.20540005e-01,  -5.37670016e-01,\n",
       "         9.36279967e-02,  -2.12109998e-01,  -1.14069998e-01,\n",
       "        -9.27169994e-02,  -1.64649993e-01,  -3.11980009e-01,\n",
       "        -2.40339994e-01,   8.75099972e-02,  -5.72189987e-02,\n",
       "         1.63609996e-01,  -1.37700001e-02,  -1.00730002e-01,\n",
       "        -7.25660026e-02,   7.22010016e-01,  -1.87820002e-01,\n",
       "        -4.40809995e-01,  -3.98570001e-01,   1.62139997e-01,\n",
       "        -7.88000003e-02,   5.36159985e-02,  -2.15709999e-01,\n",
       "         2.19009995e-01,   3.19059998e-01,  -1.92330003e-01,\n",
       "        -1.43089995e-01,  -6.80750012e-02,  -2.00629994e-01,\n",
       "         1.68880001e-01,  -4.79579985e-01,   3.31750005e-01,\n",
       "         3.42830010e-02,   3.62489998e-01,  -2.05559999e-01,\n",
       "        -5.03509998e-01,  -1.12860002e-01,   4.51879986e-02,\n",
       "         1.20970003e-01,   3.39870006e-01,   6.89950029e-05,\n",
       "         9.11180004e-02,  -2.25170001e-01,   5.86749986e-02,\n",
       "         2.36649998e-02,  -7.20129982e-02,   2.70209998e-01,\n",
       "        -3.80569994e-02,   8.86910036e-02,   3.38629991e-01,\n",
       "         2.92169988e-01,  -3.62810004e-03,   1.61939994e-01,\n",
       "        -9.93539989e-02,  -4.25639987e-01,  -3.58339995e-01,\n",
       "         2.93529987e-01,  -3.68829995e-01,  -1.03450000e-01,\n",
       "         4.09089997e-02,   2.84480006e-01,  -3.02210003e-01,\n",
       "        -3.05019994e-03,  -2.23450005e-01,   4.37400013e-01,\n",
       "        -1.43429995e-01,   9.50850025e-02,   3.39910001e-01,\n",
       "         2.17260003e-01,  -3.82330008e-02,   2.65610009e-01,\n",
       "         2.98150003e-01,   1.88909993e-01,  -7.71640018e-02,\n",
       "         6.19210005e-01,   1.40990004e-01,   1.42519996e-01,\n",
       "        -3.08519989e-01,   1.25249997e-02,  -8.00919980e-02,\n",
       "        -1.61509998e-02,  -2.00340003e-01,  -3.61829996e-02,\n",
       "         5.43860011e-02,  -1.80509999e-01,  -8.38660002e-02,\n",
       "        -2.94250008e-02,   2.18040004e-01,  -8.48020017e-02,\n",
       "        -5.26920021e-01,  -4.00700003e-01,   9.85609964e-02,\n",
       "         4.87129986e-01,   2.39810005e-01,   3.08169991e-01,\n",
       "        -2.36399993e-01,   2.29320005e-02,  -2.86100000e-01,\n",
       "         7.82980025e-02,  -8.22310001e-02,  -1.03639998e-02,\n",
       "         1.02760002e-01,  -3.51619989e-01,  -1.47210002e-01,\n",
       "         7.63290003e-02,  -2.15279996e-01,  -1.69410005e-01,\n",
       "        -3.93880010e-01,  -2.43239999e-02,   4.64419983e-02,\n",
       "        -2.35300004e-01,   5.66370003e-02,   1.03299998e-01,\n",
       "         1.11230001e-01,   2.45429993e-01,   9.00250003e-02,\n",
       "        -2.25439996e-01,  -1.78430006e-01,  -8.92430022e-02,\n",
       "         7.25660026e-01,  -2.16360003e-01,  -7.11790025e-02,\n",
       "         6.60979971e-02,  -1.92410007e-01,   1.18349999e-01], dtype=float32)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word.vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use these embeddings in Keras using an Embedding layer\n",
    "\n",
    "```\n",
    "embedding_layer = Embedding(num_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)\n",
    "```\n",
    "\n",
    "Where, EMBEDDING_DIM = 300, weights = the embedding matrix (nlp.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tons of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0000000601858707"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Sentence similarity\n",
    "\n",
    "doc1 = nlp(u'hey how are you')\n",
    "doc2 = nlp(u'hey how are you')\n",
    "doc1.similarity(doc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'INTJ', u'ADV', u'VERB', u'PRON']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## POS Tagging\n",
    "\n",
    "[w.pos_ for w in doc1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'Montreal', 3, u'GPE')\n"
     ]
    }
   ],
   "source": [
    "## Enity recognition\n",
    "\n",
    "doc = nlp(u'Montreal is such a cool place!')\n",
    "\n",
    "print(doc[0].text, doc[0].ent_iob, doc[0].ent_type_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and many more!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Tips\n",
    "\n",
    "### A small list of things that often go wrong\n",
    "(in Deep Learning)\n",
    "\n",
    "**The learning rate is too large**  \n",
    "Symptoms: training loss oscillates, and/or doesn't go down\n",
    "\n",
    "**The learning rate is too small**  \n",
    "Symptoms: training loss doesn't go down, nor does accuracy go up\n",
    "\n",
    "**Your model is taking forever to learn**  \n",
    "Symptoms: Validation and training error go down, but very slowly  \n",
    "Fix: Use batch normalization and momentum methods such as Adam and RMSProp\n",
    "\n",
    "**Your model has too many layers/parameters**  \n",
    "Symptoms: the training accuracy is almost 1, but the validation accuracy is terrible (overfitting!)   \n",
    "Fix: reduce the number of layers and hidden units\n",
    "\n",
    "**Your model has too little data**  \n",
    "Symptoms: the training accuracy is almost 1, but the validation accuracy is terrible (overfitting!)   \n",
    "Fix: create noised data, use Dropout, augment your data with other datasets\n",
    "\n",
    "**Your input is not well distributed**  \n",
    "Symptoms: extreme activation/loss values, and no learning  \n",
    "Fix: make sure that your input is in [0,1] or has mean 0 and variance 1. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "1. [Keras text classification with Spacy and word embeddings](https://github.com/fchollet/keras/blob/master/examples/pretrained_word_embeddings.py)\n",
    "2. [Sentiment Analysis with Keras](https://github.com/explosion/spaCy/blob/master/examples/deep_learning_keras.py)\n",
    "3. [Huge list of Keras examples](https://github.com/fchollet/keras/tree/master/examples)\n",
    "4. [Spacy Docs](https://spacy.io/docs/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
